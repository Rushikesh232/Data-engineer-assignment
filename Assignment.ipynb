{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weather_API:\n",
    "    \n",
    "    def __init__ (self, keyword):\n",
    "        self.keyword = keyword\n",
    "        \n",
    "    def json_print(self, obj):\n",
    "        # create a formatted string of the Python JSON object\n",
    "        with open('ap i_data.txt', 'w') as json_file:\n",
    "            json.dump(obj, json_file)\n",
    "        text = json.dumps(obj, sort_keys=True, indent=4)\n",
    "        print(text)\n",
    "        \n",
    "    def create_dataframe(self, obj):\n",
    "        \n",
    "        # creating a dataframe from nested JSON objects\n",
    "        FIELDS = [\"source.id\", \"source.name\", \"author\", \"title\", \"description\", \"url\", \"urlToImage\", \"publishedAt\", \"content\"]\n",
    "        df = pd.json_normalize(obj['articles'])\n",
    "        final_df = df[FIELDS]\n",
    "        #final_df.set_index('source.id', inplace = True)\n",
    "        display(final_df.head())\n",
    "\n",
    "    \n",
    "    def news_api(self):\n",
    "        \n",
    "        # Use the news-api to obtain articles published from\n",
    "        url = ('https://www.ci.richmond.ca.us/1404/Major-Projects'\n",
    "       'apiKey=4e70cabb80884db08524a28ac33cdc1d'.format(keyword = self.keyword))\n",
    "        \n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if (response.status_code == 200):\n",
    "            print('API call successful!')\n",
    "            json_response = response.json()\n",
    "            if(len(json_response['articles']) == 0):\n",
    "                print('No News Articles Found')\n",
    "            else:\n",
    "                \n",
    "                # Print a String in Json Format\n",
    "                self.json_print(json_response)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Create a pandas DataFrame\n",
    "                self.create_dataframe(json_response)\n",
    "                    \n",
    "        else:\n",
    "            print('Status code: ', response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Web_Scraping:\n",
    "    \n",
    "    def __init__(self, location):\n",
    "        self.location = locationZ\n",
    "        \n",
    "    def selenium_webdriver(self):\n",
    "        \n",
    "        # Start the Driver\n",
    "        driver = webdriver.Chrome(executable_path = r\"C:\\Users\\Rushi\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "        \n",
    "        # Hit the url of NASA Earth Data website and wait for 15 seconds.\n",
    "        url = ('https://www.shorelinewa.gov/government/projects-initiativess'.format(location = self.location))\n",
    "        driver.get(url)\n",
    "        time.sleep(15)\n",
    "        \n",
    "        # Driver scrolls down 25 times to load the table.\n",
    "        for i in range(0,30):\n",
    "            driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "            time.sleep(10)\n",
    "            \n",
    "        # Fetch the webpage and store in a variable.\n",
    "        webpage = driver.page_source\n",
    "        \n",
    "        # Parse the page using BeautifulSoup\n",
    "        HTMLPage = BeautifulSoup(webpage, 'html.parser')\n",
    "        \n",
    "        titles = []\n",
    "        description = []\n",
    "        links = []\n",
    "\n",
    "        for lists in HTMLPage.find_all(class_ = 'result'):\n",
    "            if (lists.span.text != '' and len(lists.find_all('p')) != 0):\n",
    "                titles.append(lists.span.text)\n",
    "                description.append(lists.find('p', class_ = '').text)\n",
    "                links.append(lists.find('p', class_ = 'search-link').text)\n",
    "        \n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(list(zip(titles, description, links)),\n",
    "               columns =['title', 'description', 'link'])\n",
    "        \n",
    "        display(df)\n",
    "        \n",
    "        # Store to csv file\n",
    "        df.to_csv('ws.csv', sep=',', index=False,header=True)\n",
    "        \n",
    "        print('Web Scraping Successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = input('Enter Keyword to be searched: ').lower()\n",
    "w_api = Weather_API(keyword)\n",
    "w_api.news_api()\n",
    "\n",
    "location = input('Enter Location: ').lower()\n",
    "ws = Web_Scraping('California.')\n",
    "ws.selenium_webdriver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
